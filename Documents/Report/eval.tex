
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation} \label{sec:evaluation}

This section describes the evaluation of Iroko using our testbed built on top of
Mininet simulator. The main goal is to compare the performance against the
existing centralized and decentralized techniques like Hedera~\cite{hedera} and
DCTCP~\cite{dctcp} respectively.

\subsection{Benchmark suite}

For evaluation purposes we are targeting a small scale data center of 16 hosts
and 20 switches with FatTree topology. We took measurements for Hedera, DCTCP,
ECMP and with Non-blocking switch on the same test setup. ECMP measurement is
considered as the base line. Non-blocking is a hypothetical single switch fully
connected setup and is considered as the theoretical maximum we are aiming to
achieve.

For traffic simulation, we are using the same traffic matrices used in
Hedera~\cite{hedera}. We took measurements for the variations of the following 3
traffic patterns:

\begin{enumerate} \item Stride$\left(i\right)$: A host with index $x$ sends to
the host with index $(x + i)mod(num hosts)$.  \item Staggered Prob $\left(EdgeP,
PodP\right)$: A host sends to another host in the same edge switch with
probability $EdgeP$, and to its same pod with probability $PodP$, and to the
rest of the network with probability $1-EdgeP - PodP$.  \item Random: A host
sends to any other host in the	network with uniform probability. We include
bijective mappings and ones where hotspots are present.  \end{enumerate}

Since we are primarily concerned about reducing the latency and packet drops
while keeping utilization at maximum, we measured bisection bandwidth and
    average queue length on switches. The experiments last for 60 seconds for
    each benchmark and we consider only the middle 40 seconds.

\subsection{Bisection Bandwidth}

\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{rate.png}
	\caption{Average bisection bandwidth}
	\label{fig:rate}
\end{figure*}

In Figure \ref{fig:rate} we show the aggregate bisection bandwidth. Iroko
consistently provides better utilization compared to ECMP and DCTCP in almost
all experiments. As expected non-blocking is providing the best performance out
of all four techniques. Utilization of non-blocking is lower for traffic
patterns with high incast. Hedera perform extremely well in all experiments
except for some like stride1, stag2$(0.5,0.3)$ where Iroko outperforms Hedera.
Interestingly enough, Hedera even outperforms non-blocking limit in stride8
experiment. ECMP does reasonably well when there is high probability of local
communication for example smaller strides, but start to deteriorate as the
stride length increases because of hash collisions.

Another interesting observation is that DCTCP gives abysmal utilization for some
experiments, for example randx2 and randx4. Further investigation into this
revealed that this is because of DCTCP's aggressive congestion window size
reduction strategy on incast. The proposed solution for incast problem is to use
dynamic buffer allocation in switches which is not implemented in our test
setup. General performance of DCTCP is not satisfactory either. For some
experiments it even under performs ECMP.  According to DCTCP paper it is
optimized for some specific data center traffic patterns like web search, soft
real-time applications, recommendation systems etc which we don't include in our
experiments.

\subsection{Drop rate}

\begin{figure*}
	\centering
	\includegraphics[width=1\linewidth]{qlen.png}
	\caption{CDF of queue lengths}
	\label{fig:qlen}
\end{figure*}

To analyze the drop rate we measured the number of packets built up in the queue
on all switches for the same set of traffic patterns and plotted a cumulative
distribution graph (Figure \ref{fig:qlen}). A high queue size is considered to
be bad. We have fixed the maximum queue size to 50 packets.

Iroko is not doing a good job here by consistently keeping $90\%$ of the queues
empty and rest $10\%$ full. But it is interesting to note that Iroko performs
really well for traffic patterns like stride1, stag2$(0.5,0.3)$ etc where it
shows high utilization as well. DCTCP maintains the queues to the minimum across
all experiments. This is not surprising considering the low utilization of
DCTCP. ECMP and Hedera behaves almost the same. Nonblocking has the highest
queue size out of all techniques. We believe that the CPU is the bottleneck here
since it only uses a single switch to connect the entire network.

Iroko definitely need more fine tuning for the prediction algorithm. We also
need to investigate why the graph has a sudden jump from $0$ to $50$. Initial
analysis shows that a small fixed set of switches are the bottleneck in all
experiments. Our guess is that this happens because of the unfair bandwidth
allocation across traffic flows. Analyzing fairness might provide better insight
into this problem.

\subsection{More Benchmarks}

Apart from utilization and loss rate following are the other benchmarks that we
are interested. We have not measured these yet due to time concerns.

\begin{enumerate}

\item \textbf{99th percentile latency:} Since we are aiming for a low latency
which means that 99th percentile latency in the network across all flows should
be as low as possible.

\item \textbf{Fairness:} Fairness across flows can be tested by introducing a
new host to a completely saturated network and increasing the transmission rate
to see if all the hosts gets fair share of the total bandwidth. Currently we are
assuming all flows should be at equal priority. Differential priority is out of
scope.

\item \textbf{Responsiveness:} This metric evaluates the predictive power and
efficiency of Iroko. It would be interesting to analyze the response time of the
central scheduler compared to decentralized DCTCP and TCP.

\item \textbf{Starvation:} Since we are rate limiting at the host level, Iroko
should make sure that no flows are starving to send data. This can be measured
by plotting residual bandwidth and excess load.

\end{enumerate}
