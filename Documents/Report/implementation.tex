

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
\subsection{Overview}

We emulate our system in Mininet~\cite{mininet} to observe traffic 
patterns and to appropriately rate-limit end-hosts. Mininet has proven itself to be 
a viable tool to model new congestion control 
algorithms~\cite{mininet_learning}, and helped us prototype our concept 
efficiently. We built our custom SDN controller Iroko which interacts with 
traditional OpenFlow software switches as well as end-hosts. End-hosts run 
a custom real-world traffic generation script which adjusts based on 
information packets sent by the controller.


\subsection{Topology Simulation}
We chose to use  Mininet, a python data center emulation API, to prototype Iroko. Mininet's implementation leverages the Linux kernel to provide a lightweight virtualization of a data center in a single machine ~\cite{mininet}. This is an important advantage, because it allowed us to create a prototype we can test using limited amounts of hardware. Other methods of testing Iroko would require more resources, or require specialized code specific to the testbed which cannot deployed in a real system ~\cite{mininet}. In contrast, Mininet has allowed us to develop Iroko to be deployable in a real system without re-implementation independent of testbed specific code. 

Mininet’s lightweight virtualization also allows the API to scale better, and to support bigger topologies which otherwise couldn’t be prototyped easily ~\cite{mininet}. This feature will be useful for future research, because we can then test Iroko on other topologies beyond the fat-tree topology we looked at in this work. 

In the Mininet environment, we created a fat-tree topology consisting of 20 switches and 16 hosts . The 20 switches are configured into 5 pods as described in ~\cite{fattree}. This design choice was to allow a direct comparison to Hedera  ~\cite{hedera}, a global arbiter which uses flow statistics to re-route elephant flows. In ~\cite{hedera}, Hedera was evaluted  using a fat-tree topology as we have described, and this allows us to directly compare Iroko against Hedera's performance . 

\subsection{Data Collection }
We collected several different metrics from the links connected to each host. We leverage the fact we know which host connects to which link in the data center. We used the awk scripting language and the Linux tc qdisc command to collect information from the links. 

Awk is a text parsing language available on most Linux distributions by default. The awk script reads the /proc/net/dev file to collect the currently used  bandwidth on a link. We then used this to calculate free bandwidth which is the difference between theoretical maximum bisection bandwidth, and the reported bandwidth on a link.

In addition, we used the following Linux command:
\[$tc -s qdisc show dev <interface>$ \] 
where $<interface>$ is the current host we are polling from. Using the obtained text, we then applied regex expressions to collect the packet loss, packet overlimits, and interface queues . Each of these data provide further information to hint at the current state of the data center. Our goal was to eliminate dropped packets, and used this to help Iroko make rate-limiting decisions. Overlimit packets provides information on when a link goes over it’s allocated bandwidth, and adds a potentially useful feature for our controller to learn from. Interface queues provide Iroko with information on the queue length of the host.  These additional statistics we hoped would provide our controller enough data to make meaningful inferences of the appropriate bandwidth allocation to each host. 

\subsection{Bandwidth Allocation}
Iroko rate limits traffic at the applications level, and thus uses UDP instead of TCP to communicate with hosts. It reads a traffic matrix shared among all the hosts and sends network traffic accordingly. Iroko also manages the current allocated bandwidth per host by sending a packet which updates their current allocated bandwidth.


\subsection{Iroko}

In order to make rate-limiting decisions per host, we treat bandwidth allocation as a reinforcement learning problem. The goal in reinforcement learning is to maximize the expected reward given by an environment for each possible state $s$ ~\cite{Sutton:1998:IRL:551283}. These values are often tracked using the state-action value function $Q(s,a)$, and often this value is approximated using a function due to intractable memory needs ~\cite{Sutton:1998:IRL:551283}. Using $Q(s,a)$, a policy $\pi(s)$ is then learned to direct how the agent should proceed in the environment.$\pi(s)$ role is to decide the next action a reinforcement learning agent should take, in our case the amount to rate-limit a host. 
 

We represent a state from the data center as an array combining a bit vector corresponding to the current host and it's allocated bandwidth, free bandwidth, packet loss, packet overlimits, and queue length:

\[ [0 0 0 ... 1 0 0, bandwidth, free bandwidth, packet loss, overlimit, queue length ] \]
The advantage of this is that Iroko will learn a policy which considers all hosts together in unified representation instead of using separate agents for each network which wouldn't consider bandwidth allocation to each separate host.

The action Iroko takes at each step is the amount to increase or decrease the current hosts allocated bandwidth. Iroko will generate an adjustment value clamped between $-.9 - .9$. We chose this value range, because we wanted to avoid starving hosts.  Iroko then modifies the allocated bandwidth similar to the approach used in ~\cite{remy} for congestion windows by using the action with allocated bandwidth as follows:

\[bandwidth_{t+1}   <-- bandwidth_t +  a_t * bandwidth_t\]

where a is the chosen bandwidth adjustment, t is the previous epoch, and t+1 is the new allocation. This representation could be  modified in a number of ways, such as forcing a minimum expected bandwidth, or add the capacity to starve a host.
 
 
For a reinforcement learning agent to learn a policy, a reward function is used to generate the value of a particular state, and is used to represent the goal of an agent ~\cite{Sutton:1998:IRL:551283}. We defined  a reward of 1 if the packet loss was 0 , and a reward of -1 for any loss greater than 0. M  A more intricate reward functions can be devised, and onea limitation of the current reward signal is that 0 loss can occuris also obtainable if no bandwidth is used in the current epoch which could confuse the agent. 



We chose to use the deep deterministic policy gradient policy algorithm (DDPG) as described in ~\cite{DDPG} . DDPG is an actor-critic model which uses neural networks to approximate $Q(s,a)$, and learns the policy by taking the actor network gradients relative to the critic $Q(s,a)$ approximations ~\cite{DDPG}.  Our agent learns using a replay memory, which is a technique that helps deep reinforcement learning algorithms learn by making the data uncorrelated via random batch sampling ~\cite{DQLearning}. This decision might be unnecessary for a data center where some authors have suggested that data center traffic is inherently unpredictable ~\cite{microte} and may suggest that each state can be viewed as uncorrelated inherently. We recognize these findings may also give an upper limit to the expected performance of Iroko. 

To find the optimal values, an agent must balance exploiting the current best actions with exploring an environment with a new action which may lead to bigger future rewards ~\cite{Sutton:1998:IRL:551283}. Instead of using the exploration approach used in ~\cite{DDPG}, we use $\epsilon-greedy\ learning$, where an agent takes the best action with probability $\epsilon$ and a random action with probability $1 - \epsilon$. Our random action is then defined as follows:

\[
noise(x) = 
\begin{cases}

uniform(0,1), &\text{P(x = increase) = .333},\\
uniform(-1,0), &\text{P(x = decrease) = .3333},\\
0, & \text{(x = unchanged) = .3333}
\end{cases}
\]

where uniform(a,b) refers to taking a random value in the range between [a, b]
This means we will take the best predicted value $(.5 * 1 + .5 * .33) =  ~.66.6\%$  and will increase $P(x= increase) * P(\epsilon) = 16. 667\%$ of the time or decrease $P(x= decrease) * P(\epsilon) = 16. 667\%$ of the time. It should be noted that the action value is still clamped between -0.9 and 0.9 regardless of the noise introduced. Alternative exploration policies could be used, but this one was chosen due it’s simplicity, and theoretical capability to explore the whole action space if allowed to run indefinitely.
