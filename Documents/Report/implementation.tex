

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
\subsection{Overview}

We emulate our system in Mininet~\cite{mininet} to observe traffic 
patterns and to appropriately rate-limit end-hosts. Mininet has proven itself to be 
a viable tool to model new congestion control 
algorithms~\cite{mininet_learning}, and helped us prototype our concept 
efficiently. We built our custom SDN controller Iroko which interacts with 
traditional OpenFlow software switches as well as end-hosts. End-hosts run 
a custom real-world traffic generation script which adjusts based on 
information packets sent by the controller.


\subsection{Topology Simulation}
We chose to use  Mininet, a python datacenter emulation API, to prototype Iroko.
Mininet's implementation leverages the Linux kernel to provide a lightweight virtualization
of a datacenter in a single machine ~\cite{mininet}. This allowed us to create a prototype of
Iroko we could test quickly using limited amounts of hardware. Other methods of testing Iroko 
would require more resources, or specialized code specific to the testbed which cannot be 
deployed in a real system ~\cite{mininet}. In contrast, Mininet has allowed us to develop
Iroko such that we could deploy it in a real system without re-implementation independent of testbed specific code. 

In the Mininet environment, we created a FatTree topology consisting of 20 switches and 16 hosts.
The 20 switches were configured into 5 pods as described in ~\cite{fattree}. This design choice was
to allow a direct comparison to Hedera  ~\cite{hedera}, a global arbiter which uses flow statistics
to re-route elephant flows. In ~\cite{hedera}, Hedera was evaluated using a FatTree topology as
we have described, and this allows us to directly compare Iroko against Hedera's performance . 

Mininet's lightweight virtualization also allows the API to scale; supporting bigger topologies which otherwise couldn't be prototyped easily ~\cite{mininet}.
This feature will be useful for future research. Using Mininet, we can test Iroko on other topologies beyond the FatTree topology. 

\subsection{Data Collection }
Since we knew prior to testing which port connected to each host,
we collected different data from port connections to each end-host. 
This was accomplished using the awk scripting language and the Linux tc qdisc command to collect information.

Awk is a text parsing language available on most Linux distributions by default.
Our awk script reads the /proc/net/dev file on each interface to collect the currently
used  bandwidth on a link. We then used this to calculate free bandwidth which is the difference
between theoretical maximum bisection bandwidth, and the reported bandwidth on a link.

In addition, we used the following Linux command:\\
\vspace{1mm}

\texttt{tc -s qdisc show dev <interface>}\\

\vspace{1mm}
where \texttt{<interface>} is the current host we are polling from. Using the 
obtained text, we then applied regex expressions to collect the packet loss, 
packet overlimits, and interface queues. Each of these data points provide further 
information about the current state of the datacenter. Our goal was to 
eliminate dropped packets. We used this technique to help Iroko make rate-limiting 
decisions. Overlimit packets provide information on when a link goes over its 
allocated bandwidth, and adds a potentially useful feature for our controller 
to learn from. Interface queues provide Iroko with information on the queue 
length of the host, which measures how many packets are buffered to be 
processed.  We hoped these additional statistics would provide our controller 
enough data to make meaningful inferences of the appropriate bandwidth 
allocation to each host. 

\subsection{Bandwidth Allocation}
Iroko rate limits traffic at the application level, and thus uses UDP instead
of TCP to communicate with hosts. Traffic is generated using a custom load generation
application. We wrote this application to read a traffic matrix shared among all the hosts and send network traffic accordingly.
Iroko manages the current allocated bandwidth per host by sending a controller packet
which updates the current allocated bandwidth.


\subsection{Reinforcement Learning Algorithm}

We chose to use the deep deterministic policy gradient
policy algorithm (DDPG) as described in ~\cite{DDPG} .
DDPG is a form of actor-critic reinforcement learning algorithm ~\cite{Sutton:1998:IRL:551283}
which uses neural networks to approximate $Q(s,a)$, and learns the policy by taking the actor
network gradients relative to the critic's $Q(s,a)$ approximations ~\cite{DDPG}.  Our agent learns using replay memory;
a technique that helps deep reinforcement learning algorithms learn by
making the data uncorrelated via random batch sampling ~\cite{DQLearning}.
This decision might be unnecessary for a datacenter as datacenter traffic may be inherently unpredictable ~\cite{microte}.
Thus we can reasonably assume datacenter traffic is inherently uncorrelated. We recognize these findings
may also give an upper limit to the expected performance of Iroko. 

To find the optimal values, an agent must balance exploiting
the current best actions with exploring an environment with 
a new action which may lead to bigger future rewards ~\cite{Sutton:1998:IRL:551283}.
Instead of the exploration approach used in ~\cite{DDPG}, we use $\varepsilon-greedy\ learning$,
where an agent takes the best action with probability $\varepsilon$ and a random action with probability
$1 - \varepsilon$ ~\cite{Sutton:1998:IRL:551283}. Our random action is defined as follows:

\[
noise = 
\begin{cases}

uniform(0,1), &\text{P(x = increase) = 1/3},\\
uniform(-1,0), &\text{P(x = decrease) = 1/3},\\
0, & \text{P(x = unchanged) = 1/3}
\end{cases}
\]
\\

where uniform(a,b) refers to taking a random value in the range between [a, b]
This means we will take the best predicted value $(.5 * 1 + .5 * .33) =  ~.66.6\%$  
and will increase $P(x= increase) * P(\varepsilon) = 16. 667\%$ of the time or
decrease $P(x= decrease) * P(\varepsilon) = 16. 667\%$ of the time.
It should be noted that the action value is still clamped between
-0.9 and 0.9 regardless of the noise introduced. Alternative exploration policies 
could be used, but this one was chosen due it's simplicity, and theoretical capability to explore the whole action space if allowed to run indefinitely.

We represent the traffic state of the datacenter as an array combining a bit vector
corresponding to the current host and its allocated bandwidth, free bandwidth, packet loss, packet overlimits, and queue length. 
The advantage of this technique is that Iroko will learn a policy which considers all hosts together 
in a unified representation.  

At each time-step, Iroko increases or decreases the hosts' allocated bandwidth. 
Iroko generates an adjustment value clamped between $-0.9$ and $0.9$. We chose this value range to avoid host starvation. 
Iroko modifies the allocated bandwidth similar to the approach used in ~\cite{remy} for congestion windows by using the action with allocated bandwidth as follows:

\[bandwidth_{t+1}   \leftarrow bandwidth_t +  a_t * bandwidth_t\]
\\
where $a$ is the chosen bandwidth adjustment, $t$ is the previous epoch, and $t+1$ is the current epoch. 
This representation could be  modified in a number of ways, for example: enforcing a minimum expected bandwidth, or adding capacity to a starved host.



