

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
\label{sec:implementation}
We intend to emulate our system in Mininet~\cite{mininet} to observe traffic 
patterns and infer a suitable token algorithm. Mininet has proven itself to be 
a viable tool to model new congestion control 
algorithms~\cite{mininet_learning}, and will help us prototype our concept 
efficiently. We will build a custom SDN controller that interacts with 
traditional OpenFlow software switches as well as end-hosts. End-hosts will run 
a custom real-world traffic generation script which adjusts based on 
information packets sent by the controller.
If time permits, we may expand our implementation to MaxiNet, which can emulate 
large-scale network stress tests on multiple physical hosts.

We initially considered a second implementation alternative in C/C++ based on 
the FastPass~\cite{fastpass} source code. This would provide us with a fully 
deployable system which we could fork our implementation from. A major 
advantage of this approach is the ability to test scenarios and traffic 
algorithms using real software code.
However, several concerns made us favour a Mininet emulation instead.
Firstly, FastPass relies on DPDK integration, which requires actual hardware 
interfaces. The central arbiter in the FastPass design would need to run on a 
dedicated machine, which increases prototyping and development complexity 
substantially.

Secondly, the FastPass code is highly specialized and optimized research code 
with only little available documentation. Modifying and evolving the source 
code will require thorough understanding of kernel and networking development, 
a significant time-sink. For a class project, these may be major initial 
hurdles, taking away from the research aspect of the design concept.
Consequently, we have decided to pursue an approach which allows us to quickly 
develop an understanding of the problem without being obstructed by engineering 
work.

\subsection{Topology Simulation}
We chose to use  Mininet, a python data center emulation API, to prototype Iroko. Mininet's implementation leverages the Linux kernel to provide a lightweight virtualization of a data center in a single machine ~\cite{mininet}. This is an important advantage, because it allows us to create a prototype we can test using limited amounts of hardware. Other methods of testing Iroko would require more resources, or require specialized code specific to the testbed which cannot deployed in a real system ~\cite{mininet}. Mininet avoids these problems and allowed us to develop a prototype of Iroko which could be deployed into a real data center. 

Mininet’s lightweight virtualization also allows the API to scale better, and to support bigger topologies which otherwise couldn’t be prototyped easily ~\cite{mininet}. This feature will be useful for future research, because we can then test Iroko on other topologies beyond the ones we look use in this current work. 

We created  fat-tree topology consisting of 20 switches and 16 hosts in the Mininet environment. The 20 switches are configured into 5 pods as described in ~\cite{fattree}. This design choice was to allow a direct comparison to Hedera  ~\cite{hedera}, a global arbiter which uses flow statistics to re-route elephant flows. In their work they also used a fat-tree topology as we have described, and this allows us to directly evaluate against Hedera's performance . 

\subsection{Data Collection }
We collected several different metrics from the links connected to each host. We leverage the fact we know which host connects to which link in the data center. We used the awk scripting language and the Linux tc qdisc command to collect information from the links. 

Awk is a text parsing language available on most Linux distributions by default. Our script read from each links. The awk script read the /proc/net/dev file to collect the currently used  bandwidth on a link. We then used this to calculate free bandwidth which is the difference between theoretical maximum bisection bandwidth for a link from the used bandwidth.

In addition, we used $tc -s qdisc show dev <interface>$ where $<interface>$ is the current host we are polling from, and a combination of regex expressions to collect the packet loss, packet overlimits, and interface queues . Each of these data provide further information to hint at the current state of the data center. Our goal was to eliminate dropped packets, and use this to help Iroko make predictions. Overlimit packets provides information on when a link goes over it’s allocated bandwidth, and adds a potentially useful feature for our controller to learn from. Interface queues provide Iroko with information on the queue length of the host.  These additional statistics we hoped would provide our controller enough data to make meaningful inferences of the appropriate bandwidth allocation to each host. 

\subsection{Bandwidth Allocation}
Iroko rate limits traffic at the applications level, and thus uses UDP instead of TCP to communicate with hosts. It reads a traffic matrix shared among all the hosts and sends network traffic accordingly. Iroko also manages the current allocated bandwidth per host by sending a packet which updates their current allocated bandwidth.


\subsection{Iroko}

In order to make inferences about the appropriate bandwidth allocation per host, we treat bandwidth allocation as a reinforcement learning problem. The goal in reinforcement learning is to maximize the expected reward given by an environment for each possible state $s$ ~\cite{Sutton:1998:IRL:551283}. These values are often tracked using the state-action value function $Q(s,a)$, but often it is intractable to store all $(s,a)$) pairs a table, and often a function approximator is used instead to estimate $Q(s,a)$. Using function $Q(s,a)$, a policy $\pi(s)$ is then learned to direct how the agent should proceed in the environment.
 

We represent a state from the data center as an array combining a bit vector corresponding to the current host and it's allocated bandwidth, free bandawidth, packet loss, packet overlimits, and queue length:

\[ [0 0 0 ... 1 0 0, bandwidth, free bandwidth, packet loss, overlimit, queue ] \]
The advantage of this is that Iroko will learn a policy which considers all hosts together in a representation instead of using separate agents for each network which wouldn't consider bandwidth allocation to each separate host.

The action Iroko takes at each step is the amount to increase or decrease the current hosts allocated bandwidth. Iroko will generate an adjustment value clamped between $-.9 - .9$. We chose this value range, because we wanted to avoid starving hosts.  Iroko then modifies the allocated bandwidth similar to the approach used in ~\cite{remy} for congestion windows by using the action with allocated bandwidth as follows:

\[bandwidth_{t+1}   <-- bandwidth_t +  a_t * bandwidth_t\]

where a is the chosen bandwidth adjustment, t is the previous epoch, and t+1 is the new allocation. This representation could be  modified in a number of ways, such as forcing a minimum expected bandwidth, or add the capacity to starve a host.
 
 
For a reinforcement learning agent to learn a policy, a reward function is used to generate the value of a particular state, and is used to represent the goal of an agent ~\cite{Sutton:1998:IRL:551283}. We defined  a reward of 1 if the packet loss was 0 , and a reward of -1 for any loss greater than 0. M  A more intricate reward functions can be devised, and onea limitation of the current reward signal is that 0 loss can occuris also obtainable if no bandwidth is used in the current epoch which could confuse the agent. 



We chose to use the deep deterministic policy gradient policy algorithm (DDPG) as described in ~\cite{DDPG} . DDPG is an actor-critic model which uses neural networks to approximate $Q(s,a)$, and learns the policy by taking the actor network gradients relative to the critic $Q(s,a)$ approximations ~\cite{DDPG}.  Our agent learns using a replay memory, which is a technique that helps deep reinforcement learning algorithms learn by making the data uncorrelated via random batch sampling ~\cite{DQLearning}. This decision might be unnecessary for a data center where some authors have suggested that data center traffic is inherently unpredictable ~\cite{microte} and may suggest that each state can be viewed as uncorrelated inherently. We recognize these findings may also give an upper limit to the expected performance of Iroko. 

To find the optimal values, an agent must balance exploiting the current best actions with exploring an environment with a new action which may lead to bigger future rewards ~\cite{Sutton:1998:IRL:551283}. Instead of using the exploration approach used in ~\cite{DDPG}, we use $\epsilon-greedy\ learning$, where an agent takes the best action with probability $\epsilon$ and a random action with probability $1 - \epsilon$. Our random action is then defined as follows:

\[
noise(x) = 
\begin{cases}

uniform(0,1), &\text{P(x = increase) = .333},\\
uniform(-1,0), &\text{P(x = decrease) = .3333},\\
0, & \text{(x = unchanged) = .3333}
\end{cases}
\]

where uniform(a,b) refers to taking a random value in the range between [a, b]
This means we will take the best predicted value $(.5 * 1 + .5 * .33) =  ~.66.6\%$  and will increase $P(x= increase) * P(\epsilon) = 16. 667\%$ of the time or decrease $P(x= decrease) * P(\epsilon) = 16. 667\%$ of the time. It should be noted that the action value is still clamped between -0.9 and 0.9 regardless of the noise introduced. Alternative exploration policies could be used, but this one was chosen due it’s simplicity, and theoretical capability to explore the whole action space if allowed to run indefinitely.
