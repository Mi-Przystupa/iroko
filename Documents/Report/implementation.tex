

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
\label{sec:implementation}
We intend to emulate our system in Mininet~\cite{mininet} to observe traffic 
patterns and infer a suitable token algorithm. Mininet has proven itself to be 
a viable tool to model new congestion control 
algorithms~\cite{mininet_learning}, and will help us prototype our concept 
efficiently. We will build a custom SDN controller that interacts with 
traditional OpenFlow software switches as well as end-hosts. End-hosts will run 
a custom real-world traffic generation script which adjusts based on 
information packets sent by the controller.
If time permits, we may expand our implementation to MaxiNet, which can emulate 
large-scale network stress tests on multiple physical hosts.

We initially considered a second implementation alternative in C/C++ based on 
the FastPass~\cite{fastpass} source code. This would provide us with a fully 
deployable system which we could fork our implementation from. A major 
advantage of this approach is the ability to test scenarios and traffic 
algorithms using real software code.
However, several concerns made us favour a Mininet emulation instead.
Firstly, FastPass relies on DPDK integration, which requires actual hardware 
interfaces. The central arbiter in the FastPass design would need to run on a 
dedicated machine, which increases prototyping and development complexity 
substantially.

Secondly, the FastPass code is highly specialized and optimized research code 
with only little available documentation. Modifying and evolving the source 
code will require thorough understanding of kernel and networking development, 
a significant time-sink. For a class project, these may be major initial 
hurdles, taking away from the research aspect of the design concept.
Consequently, we have decided to pursue an approach which allows us to quickly 
develop an understanding of the problem without being obstructed by engineering 
work.

\subsection{Topology Simulation}
We chose to use  Mininet to prototype Iroko. Mininet is a python emulation API which leverages the linux kernel to provide a lightweight virtualization of a data center in a single machine ~\cite{mininet}. This is an important advantage, because it allows us to get meaningful results with limited amounts of hardware. Other methods of testing Iroko would require more resources, or require specialized code specific to the testbed. Mininet avoids these problems and allowed us to develop a prototype which could be deployed into a real data center. 

Mininet’s lightweight virtualization also allows the API to scale to bigger topologies which otherwise couldn’t be prototyped easily ~\cite{mininet}. This feature will be useful for further future research, because we can then test our controller on other topologies beyond the ones we look in this current work. 

We created  fat-tree topology consisting of 20 switches and 16 hosts in the Mininet environment. The 20 switches are configured into pods as described in < cite paper about Fat-tree>. This design choice was to allow a direct comparison to Hedera  ~\cite{hedera}, another global arbiter which re-routes elephant flows using information from the data center.  In their work they also used a fat-tree topology as we have described. This topology could be further expanded in future work to determine the scalability of our controller. 

\subsection{Data Collection }
	Using a mapping from links to hosts, we were able to directly infer information from a host using data collected from the links in the second tier of our topology. We used the awk scripting language and the linux tc qdisc command to collect information from the links. 
Awk is a built in text parsing language available on most Linux distributions < cite?>. We used a script which would read the /proc/net/dev file to collect the currently used  bandwidth on a link. From this information, we calculate the free bandwidth as the difference between the theoretical maximum bi-section link bandwidth and the used bandwidth . As our goal was to predict the appropriate bandwidth allocatio to each host, we assumed it to be useful data to the controller to know the current bandwidth utilization by a host. 
Using tc -s qdisc show dev <interface> and a combination of regex expressions, we collected the packet loss, interface overlimits, and interface queues. Each of these data provide further information to hint at the current state of the data center. Our goal was to eliminate dropped packets, and would  use this measurement of our controller performance. Overlimit packets provides information on when a link goes over it’s allocated bandwidth, and adds a potentially useful feature for our controller to learn from. Interface queues provide the controller with information on the queue length of the host.  These additional statistics we hoped would provide our controller enough data to make meaningful inferences of the appropriate bandwidth allocation to each host. 
\subsection{Bandwidth Allocation}
We implmented an applications level set of code which connects with our python code base via Sockets. It works by sending a packet to hosts which allocated their allowed maximum bandwidth for an epoch. It reads a traffic matrix shared among all the hosts and sends traffic accordingly. For Iroko we are rate limiting the traffic at the application level. So we are using UDP instead of TCP.


\subsection{Iroko}

The core of Irokos learning capabilities is solved using a reinforcement learning algorithm. Reinforcement learning is a research field in machine learning which develops techniques which can solve the reinforcement learning problem. The goal in reinforcement learning is to learn what decisions to make that maximize the expected future reward from the environment based on it’s current state. An agent follows a policy, a function which maps to decisions the agent can make in the environment, which allows the agent to explore the environment. To adjusts it’s decision, a reward function is used to determine how valuable the current state is to the agent ~\cite{Sutton:1998:IRL:551283}. The reward is generally used to represent the goal the agent is trying to achieve ~\cite{Sutton:1998:IRL:551283}. 

We decided to map a “state” in our data center as the current host to allocate bandwidth, along with all the associated data we decribed in the previous section. This representation allowed us to use a single controller for all the hosts. The advantage of this is that our controller could learn on all hosts in a single model instead of separate models per host which would have looked at each host’s bandwidth allocation in isolation. The action of our agent was the percentage to either increase or decrease the current hosts allocated bandwidth. We chose to use a single action which could take values betwen -.9 and .9.  We chose this representation, because we assumed that a host should alway have some allocated bandwidth. The current implementation also adjust bandwidth based on the current allocated bandwidth, and could thus forseeably elimnate a hots bandwidth entirely. We defined  a reward of 1 if the packet loss was 0 , and a reward of -1 for any loss greater than 0.   A more intricate reward function can be devised, and a limitation of the current reward signal is that 0 loss is also obtainable if no bandwidth is used in the current epoch. 



We chose to use the deep deterministic policy gradient policy algorithm as described in ~\cite{DDPG} . DDPG is an actor-critic model which learns the appropriate policy by taking the gradient of the actor network relative to the critic Q-value approximation.  Our agent learns using a replay memory implemented as a circular array. A replay memory buffer has shown to help deep learning algorithms learn better by sampling state, action, reward, transition state pair to learn from which helps speed learning ~\cite{DQLearning}. The motivation for a replay buffer is to train on data which is uncorrelated, which is not normally the case in reinforcement learning problems. This decision might be unnecessary for a data center where some authors have suggested that data center traffic is unpredictable and may suggest that each state can then be seen to be independent, and identifcally sampled from the environment. We recognize these findings may also give an upper limit to the expected succes of Iroko. 

To learn, a reinforcement learning agent must balance exploration and exploitation ~\cite{Sutton:1998:IRL:551283}. Instead of using the exploration policy suggested in ~\cite{DDPG} we add uniform distributed value between 0 – 1 which will increase the predicted action with a 33% probability, decrease it with 33% probability, or not alter the value at all. This means we will take the predicted value (.5 * 1 + .5 * .33) =  ~.666% or 2/3 of the time, and will increase 16. 667 % of the time or decrease 16.667 % of the time. Alternative exploration policies could be used, but this one was chosen for it’s simplicity. 

Iroko then modifies the allocated bandwidth by taking the chosen action as follows:
new bandwidth <-- current bandwidth + (suggested allocation ) * current bandwidth