

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
\subsection{Overview}

We emulate our system in Mininet~\cite{mininet} to observe traffic 
patterns and to appropriately rate-limit end-hosts. Mininet has proven itself to be 
a viable tool to model new congestion control 
algorithms~\cite{mininet_learning}, and helped us prototype our concept 
efficiently. We built our custom SDN controller Iroko which interacts with 
traditional OpenFlow software switches as well as end-hosts. End-hosts run 
a custom real-world traffic generation script which adjusts based on 
information packets sent by the controller.


\subsection{Topology Simulation}
We chose to use  Mininet, a python data center emulation API, to prototype Iroko. Mininet's implementation leverages the Linux kernel to provide a lightweight virtualization of a data center in a single machine ~\cite{mininet}. This allowed us to create a prototype of Iroko we can test quickly using limited amounts of hardware. Other methods of testing Iroko would require more resources, or specialized code specific to the testbed which cannot be deployed in a real system ~\cite{mininet}. In contrast, Mininet has allowed us to develop Iroko such that we could deploy it in a real system without re-implementation independent of testbed specific code. 

In the Mininet environment, we created a fat-tree topology consisting of 20 switches and 16 hosts . The 20 switches are configured into 5 pods as described in ~\cite{fattree}. This design choice was to allow a direct comparison to Hedera  ~\cite{hedera}, a global arbiter which uses flow statistics to re-route elephant flows. In ~\cite{hedera}, Hedera was evaluated  using a fat-tree topology as we have described, and this allows us to directly compare Iroko against Hedera's performance . 

Mininet’s lightweight virtualization also allows the API to scale such as to support bigger topologies which otherwise couldn’t be prototyped easily ~\cite{mininet}. This feature will be useful for future research, because we can then test Iroko on other topologies beyond the fat-tree topology we looked at in this work. 

\subsection{Data Collection }
Since we knew prior to testing which port connected to each host, we collected different data from port connections to each end-host. This was accomplished using the awk scripting language and the Linux tc qdisc command to collect information.

Awk is a text parsing language available on most Linux distributions by default. Our awk script reads the /proc/net/dev file on each interface to collect the currently used  bandwidth on a link. We then used this to calculate free bandwidth which is the difference between theoretical maximum bisection bandwidth, and the reported bandwidth on a link.


In addition, we used the following Linux command:\\
\texttt{tc -s qdisc show dev <interface>}\\
where \texttt{<interface>} is the current host we are polling from. Using the 
obtained text, we then applied regex expressions to collect the packet loss, 
packet overlimits, and interface queues. Each of these data provide further 
information to hint at the current state of the data center. Our goal was to 
eliminate dropped packets, and used this to help Iroko make rate-limiting 
decisions. Overlimit packets provides information on when a link goes over it’s 
allocated bandwidth, and adds a potentially useful feature for our controller 
to learn from. Interface queues provide Iroko with information on the queue 
length of the host, which measures how many packets are buffered to be 
processed.  These additional statistics we hoped would provide our controller 
enough data to make meaningful inferences of the appropriate bandwidth 
allocation to each host. 

\subsection{Bandwidth Allocation}
Iroko rate limits traffic at the applications level, and thus uses UDP instead of TCP to communicate with hosts. It reads a traffic matrix shared among all the hosts and sends network traffic accordingly. Iroko also manages the current allocated bandwidth per host by sending a packet which updates their current allocated bandwidth.


\subsection{Reinforcement Learning Algorithm}

We chose to use the deep deterministic policy gradient policy algorithm (DDPG) almost entirely as described in ~\cite{DDPG} . DDPG is a form of actor-critic reinforcement learning algorithm ~\cite{Sutton:1998:IRL:551283} which uses neural networks to approximate $Q(s,a)$, and learns the policy by taking the actor network gradients relative to the critic $Q(s,a)$ approximations ~\cite{DDPG}.  Our agent learns using replay memory, which is a technique that helps deep reinforcement learning algorithms learn by making the data uncorrelated via random batch sampling ~\cite{DQLearning}. This decision might be unnecessary for a data center where authors have suggested that data center traffic is inherently unpredictable ~\cite{microte} and this suggests that we could assume data center data is inherently uncorrelated. We recognize these findings may also give an upper limit to the expected performance of Iroko. 

To find the optimal values, an agent must balance exploiting the current best actions with exploring an environment with a new action which may lead to bigger future rewards ~\cite{Sutton:1998:IRL:551283}. Instead of using the exploration approach used in ~\cite{DDPG}, we use $\epsilon-greedy\ learning$, where an agent takes the best action with probability $\epsilon$ and a random action with probability $1 - \epsilon$ ~\cite{Sutton:1998:IRL:551283}. Our random action is then defined as follows:

\[
noise(x) = 
\begin{cases}

uniform(0,1), &\text{P(x = increase) = .333},\\
uniform(-1,0), &\text{P(x = decrease) = .3333},\\
0, & \text{(x = unchanged) = .3333}
\end{cases}
\]

where uniform(a,b) refers to taking a random value in the range between [a, b]
This means we will take the best predicted value $(.5 * 1 + .5 * .33) =  ~.66.6\%$  and will increase $P(x= increase) * P(\epsilon) = 16. 667\%$ of the time or decrease $P(x= decrease) * P(\epsilon) = 16. 667\%$ of the time. It should be noted that the action value is still clamped between -0.9 and 0.9 regardless of the noise introduced. Alternative exploration policies could be used, but this one was chosen due it’s simplicity, and theoretical capability to explore the whole action space if allowed to run indefinitely.

We represent a state from the data center as an array combining a bit vector corresponding to the current host and it's allocated bandwidth, free bandwidth, packet loss, packet overlimits, and queue length:

\[ [0 0 0 ... 1 ... 0 0, bandwidth, free bandwidth, packet loss, overlimit, queue length ] \]
The advantage of this is that Iroko will learn a policy which considers all hosts together in unified representation instead of using separate agents for each network which wouldn't consider bandwidth allocation to each separate host.

The action Iroko takes at each step is the amount to increase or decrease the current hosts allocated bandwidth. Iroko will generate an adjustment value clamped between $-.9 - .9$. We chose this value range, because we wanted to avoid host starvation.  Iroko then modifies the allocated bandwidth similar to the approach used in ~\cite{remy} for congestion windows by using the action with allocated bandwidth as follows:

\[bandwidth_{t+1}   \leftarrow bandwidth_t +  a_t * bandwidth_t\]

where a is the chosen bandwidth adjustment, t is the previous epoch, and t+1 is the new allocation. This representation could be  modified in a number of ways, such as forcing a minimum expected bandwidth, or add the capacity to starve a host.



