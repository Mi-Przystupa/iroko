
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction / Background}
\label{sec:intro}

%Initially, research in network congestion was dominated by the assumption of a 
%decentralized, autonomous network - end-hosts only have control over the 
%amount 
%of traffic they send, and are unaware of the intentions or traffic rates of 
%their peers. Similarly, switches and routers are unaware of global traffic 
%patterns and only forward based on their local notion of optimality.
%
%In line with these assumptions, TCP was designed to optimize traffic globally 
%on a simple fairness principle: nodes react to packet loss and assume that 
%others will behave accordingly. An equilibrium is reached when all end-hosts 
%achieve a traffic rate that, in sum, conforms to the maximum available 
%bandwidth of the most congested link.
%TCP works well  in scenarios where many different distrustful participants 
%compete for limited bandwidth. Still, TCP is a \textit{reactive protocol}; the 
%fact that packet loss and latency increases occur in the network already 
%indicates a problem. Packet loss is primarily due to overflowing queues in 
%forwarding elements or mismatched hardware capabilities, implying that traffic 
%has not been optimally distributed. Ideally, a network should always be 
%"zero-queue", i.e., latency will merely be induced by propagation, and not 
%queuing delay.
%
%Queueing has commonly not been a dominating issue in wide-area and enterprise 
%networks, as traffic is sufficiently distributed and diverse, with only few 
%"hot" target hosts.~\cite{hedera, microte} Traffic optimization is a 
%substantial challenge; network operators have no control over the individual 
%network elements nor its participants. Under these conditions, TCP and its 
%extensions can be considered a best-effort solution to globally maximize 
%utilization.

Developments in the past decade have changed the general networking 
environment. Data centers have emerged as an exciting new research frontier, 
posing novel design challenges and opportunities. Driven by minimization of 
costs and maximization of compute power, data centers must run at the highest 
possible utilization to achieve the ideal compute/cost ratio.
Optimizing the distribution of traffic and simultaneously guaranteeing fairness 
is a perennial challenge for any network operator.
Inefficient routing can quickly lead to bufferbloat~\cite{bufferbloat} and the 
eventual collapse of a high-load network, requiring sophisticated 
approaches to solve congestion control.
Despite the innovation potential of data centers, TCP has dominated as 
the congestion control protocol of choice.
TCP is a \textit{reactive protocol}, responding to indicators of congestion and 
latency in the network. However, the fact that packet loss and latency 
surges occur in the network, already portends a problem. Packet loss is 
incurred by overflowing queues in forwarding elements or mismatched 
hardware capabilities, implying that traffic has not been optimally 
distributed. Ideally, a network should always be "zero-queue", i.e., latency 
will merely be induced by propagation, and not queuing delay.

With the advent of Software-Defined Networking (SDN), operators now have the 
ability to freely control and adapt their network architecture, leading to 
highly customized systems and fine-grained optimization.~\cite{sdn_road}

Moving away from the principle of distributed communication and routing, SDN 
introduces the notion of "centralized management". A single controller with 
global knowledge is able to automatically modify and adapt the forwarding 
tables of all switches in the network, while notifying end hosts of changes in 
the network.
These two new trends in system design; full architectural control and 
centralized management, facilitated new opportunities in the space of TCP 
congestion research. Traffic can now be managed in a  \textit{centralized} 
fashion based on \textit{global knowledge} of the entire topology and traffic 
patterns.

A new line of centralized schedulers has emerged that can achieve close to 
optimal bandwidth utilization.~\cite{hedera, fastpass, microte, b4, dionysus}
However, these schedulers are still  \textit{reactive}  in nature. The central 
controller responds to changes in the network or requests by applications, 
which may cost valuable round-trip latency. Often, short-term flows or bursts 
are unaccounted for, which causes undesirable packet loss and back-propagating 
congestion.



The idea of admission control and service guarantees in networks is not 
new.~\cite{access_limit, access_limit2}. However, such designs traditionally 
aim to assure quality and bandwidth guarantees in a contentious, decentralized, 
and untrusted environments such as the internet. In a datacenter, these 
conditions do not apply. End-hosts are generally considered reliable and 
restricted in behavior, which allows for great simplification of enforcement 
and prioritization policies.

A desirable solution is a global, centralized arbiter which is able to predict 
and fairly distribute flows in the network before bursts or congestion even 
occur. By treating the network's compute and forwarding power as a single 
finite resource, a controller acts akin to a OS scheduler distributing CPU time 
slices to processes. This design approach follows SDN's aspiration of 
introducing operating systems abstractions to the networking domain space.

In this project, we plan to explore the possibilities of a centralized, 
proactive flow scheduler. We ask ourselves the following research questions:
\begin{enumerate}
    \item What are the requirements for such a centralized, predictive 
    scheduler to succeed?
    \item Is it possible to preemptively regulate a network by analyzing global 
    traffic patterns?
    \item What latency, packet loss and utilization are we able to achieve?
\end{enumerate}

In the scope of this course, we attempt to answer these questions and design a 
simple predictive scheduler in Mininet. If successful, we will benchmark our 
results and evaluate the level of utilization compared to contemporary 
scheduling systems.