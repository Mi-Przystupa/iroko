
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

%Initially, research in network congestion was dominated by the assumption of a 
%decentralized, autonomous network - end-hosts only have control over the 
%amount 
%of traffic they send, and are unaware of the intentions or traffic rates of 
%their peers. Similarly, switches and routers are unaware of global traffic 
%patterns and only forward based on their local notion of optimality.
%
%In line with these assumptions, TCP was designed to optimize traffic globally 
%on a simple fairness principle: nodes react to packet loss and assume that 
%others will behave accordingly. An equilibrium is reached when all end-hosts 
%achieve a traffic rate that, in sum, conforms to the maximum available 
%bandwidth of the most congested link.
%TCP works well  in scenarios where many different distrustful participants 
%compete for limited bandwidth. Still, TCP is a \textit{reactive protocol}; the 
%fact that packet loss and latency increases occur in the network already 
%indicates a problem. Packet loss is primarily due to overflowing queues in 
%forwarding elements or mismatched hardware capabilities, implying that traffic 
%has not been optimally distributed. Ideally, a network should always be 
%"zero-queue", i.e., latency will merely be induced by propagation, and not 
%queuing delay.
%
%Queueing has commonly not been a dominating issue in wide-area and enterprise 
%networks, as traffic is sufficiently distributed and diverse, with only few 
%"hot" target hosts.~\cite{hedera, microte} Traffic optimization is a 
%substantial challenge; network operators have no control over the individual 
%network elements nor its participants. Under these conditions, TCP and its 
%extensions can be considered a best-effort solution to globally maximize 
%utilization.

Developments in the past decade have changed the general networking 
environment. Data centers have emerged as an exciting new research frontier, 
posing novel design challenges and opportunities. Driven by minimization of 
costs and maximization of compute power, data centers must run at the highest 
possible utilization to achieve the ideal compute/cost ratio.
Optimizing the distribution of traffic and simultaneously guaranteeing fairness 
is a perennial challenge for any network operator.

Inefficient routing and congestion control can quickly lead to 
bufferbloat~\cite{bufferbloat} and the eventual collapse of a high-load 
network, requiring more sophisticated solutions. In fact, recent insights into 
datacenter network traffic have shown that buffer overruns and small 
short-lived bursts are the primary factor of loss.\cite{fb_dc,msr_dc}
Despite the innovation potential of data centers, TCP has dominated as 
the congestion control protocol of choice.
TCP is a \textit{reactive protocol}, responding to indicators of packet drops
latency in the network. However, the fact that loss and latency 
surge in the network, already portends a problem. Overflowing queues in 
forwarding elements or mismatched hardware capabilities imply that traffic has 
not been optimally managed.
Ideally, a network should always be "zero-queue", i.e., latency 
will merely be induced by propagation, and not queuing delay.

Recent research insights and contributions may aid in achieving such a data 
center network.


\paragraph{Software-Defined Networking}
With the advent of Software-Defined Networking (SDN), operators now have the 
ability to freely control and adapt their network architecture, leading to 
highly customized systems and fine-grained optimization.~\cite{sdn_road}

Moving away from the principle of distributed communication and routing, SDN 
introduces the notion of "centralized management". A single controller with 
global knowledge is able to automatically modify and adapt the forwarding 
tables of all switches in the network, while notifying end hosts of changes in 
the network.
This full architectural control and centralized management facilitated new 
opportunities in the space of TCP congestion research. Traffic can now be 
managed in a  \textit{centralized} fashion based on \textit{global knowledge} 
of the entire topology and traffic patterns.

\paragraph{Centralized Scheduling and Congestion Control}
Following the rise of SDN, a new line of centralized schedulers that can 
achieve close to optimal bandwidth utilization has emerged.~\cite{hedera, 
fastpass, microte, b4, dionysus}
However, these schedulers are still \textit{reactive} in nature. The central 
controller responds to changes in the network or requests by applications, 
which may cost valuable round-trip latency. Often, short-term flows or bursts 
are unaccounted for, which causes undesirable packet loss and back-propagating 
congestion.\cite{perc}

\paragraph{Admission-control systems}
In parallel, admission-control-based approaches have gained traction as a 
viable alternative to the conventional burst-and-backoff mechanism of 
TCP.~\cite{expresspass, fastpass, perc}
The idea of admission control and service guarantees in networks is not 
new.~\cite{access_limit, access_limit2}. These designs traditionally 
aim to assure quality and bandwidth guarantees in a contentious, decentralized, 
and untrusted environments such as the internet. In a datacenter, these 
conditions do not apply. End-hosts are generally considered reliable and 
restricted in behaviour, which allows for great simplification of enforcement 
and prioritization policies. Admission control systems give administrators the 
benefit of tight control over the datacenter, mitigating the effects of 
queue-buildup and bursts.

\paragraph{Predictability of traffic}
Remy\cite{remy} has shown that it is possible to learn optimal congestion 
control parameters for various network topologies, but 
Due the scarcity of public data center traffic sets, it is unclear how 
predictable traffic truly is. 


A desirable solution is a global, centralized arbiter, able to predict and 
fairly distribute flows in the network before bursts or congestion even occur. 
By treating the network's compute and forwarding power as a single finite 
resource, a controller acts akin to a OS scheduler distributing CPU time slices 
to processes.

In this project, we plan to explore the possibilities of a centralized, 
proactive flow scheduler. We ask ourselves the following research questions:
\begin{enumerate}
    \item What are the requirements for such a centralized, predictive 
    scheduler to succeed?
    \item Is it possible to preemptively regulate a network by analyzing global 
    traffic patterns?
    \item What latency, packet loss and utilization are we able to achieve?
\end{enumerate}

In the scope of this course, we attempt to answer these questions and design a 
simple predictive scheduler in Mininet. If successful, we will benchmark our 
results and evaluate the level of utilization compared to contemporary 
scheduling systems.