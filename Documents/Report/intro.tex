
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

%Initially, research in network congestion was dominated by the assumption of a 
%decentralized, autonomous network - end-hosts only have control over the 
%amount 
%of traffic they send, and are unaware of the intentions or traffic rates of 
%their peers. Similarly, switches and routers are unaware of global traffic 
%patterns and only forward based on their local notion of optimality.
%
%In line with these assumptions, TCP was designed to optimize traffic globally 
%on a simple fairness principle: nodes react to packet loss and assume that 
%others will behave accordingly. An equilibrium is reached when all end-hosts 
%achieve a traffic rate that, in sum, conforms to the maximum available 
%bandwidth of the most congested link.
%TCP works well  in scenarios where many different distrustful participants 
%compete for limited bandwidth. Still, TCP is a \textit{reactive protocol}; the 
%fact that packet loss and latency increases occur in the network already 
%indicates a problem. Packet loss is primarily due to overflowing queues in 
%forwarding elements or mismatched hardware capabilities, implying that traffic 
%has not been optimally distributed. Ideally, a network should always be 
%"zero-queue", i.e., latency will merely be induced by propagation, and not 
%queuing delay.
%
%Queueing has commonly not been a dominating issue in wide-area and enterprise 
%networks, as traffic is sufficiently distributed and diverse, with only few 
%"hot" target hosts.~\cite{hedera, microte} Traffic optimization is a 
%substantial challenge; network operators have no control over the individual 
%network elements nor its participants. Under these conditions, TCP and its 
%extensions can be considered a best-effort solution to globally maximize 
%utilization.

Developments in the past decade have changed the general networking 
environment. Data centers have emerged as an exciting new research frontier, 
posing novel design challenges and opportunities. Driven by minimization of 
costs and maximization of compute power, data centers must run at the highest 
possible utilization to achieve the ideal compute/cost ratio.
Optimizing the distribution of traffic and simultaneously guaranteeing fairness 
is a perennial challenge for any network operator.

Inefficient routing and congestion control can quickly lead to 
bufferbloat~\cite{bufferbloat} and the eventual collapse of a high-load 
network, requiring more sophisticated solutions. In fact, recent insights into 
datacenter network traffic have shown that buffer overruns and small 
short-lived bursts are the primary factor of loss.\cite{fb_dc,msr_dc,dctcp}
Despite the innovation potential of data centers, TCP has dominated as 
the congestion control algorithm of choice.
TCP is a \textit{reactive protocol}, responding to indicators of packet drops
latency. However, this surge of loss and latency in the network already 
portends a problem. Overflowing queues in forwarding elements or mismatched 
hardware capabilities imply that traffic has not been optimally managed.
Ideally, a network should always be "zero-queue", i.e., latency 
will merely be induced by propagation, and not queuing delay.

TCP's algorithm as best-effort solution to globally maximize utilization has 
not fundamentally changed over the last thirty years. However, new research 
advancement raise the question if TCP and its assumptions about networks are 
still a viable option.~\cite{pcc,bbr,perc}

With the advent of Software-Defined Networking (SDN), operators now have the 
ability to freely control and adapt their network architecture, leading to 
highly customized systems and fine-grained optimization.~\cite{sdn_road}

Moving away from the principle of distributed communication and routing, SDN 
introduces the notion of "centralized management". A single controller with 
global knowledge is able to automatically modify and adapt the forwarding 
tables of all switches in the network, while notifying end hosts of changes in 
the network.
This full architectural control facilitated new opportunities in the space of 
TCP congestion research. Traffic can now be managed in a  \textit{centralized} 
fashion based on \textit{global knowledge} of the entire topology and traffic 
patterns.

Following SDN's rise in popularity, a new line of centralized schedulers that 
can achieve close to optimal bandwidth utilization has emerged.~\cite{hedera, 
fastpass, microte, b4, dionysus}
However, these schedulers are still \textit{reactive} in nature. The central 
controller responds to changes in the network or requests by applications, 
which may cost valuable round-trip latency. Often, short-term flows or bursts 
are unaccounted for, which causes undesirable packet loss and back-propagating 
congestion.~\cite{perc}

In parallel, admission-control-based approaches have gained traction as a 
viable alternative to the conventional burst-and-backoff mechanism of 
TCP.~\cite{expresspass, fastpass, perc}
While idea of admission control and service guarantees in networks is not 
new~\cite{access_limit, access_limit2}, these designs traditionally 
aim to assure quality and bandwidth guarantees in a contentious, decentralized, 
and untrusted environments such as the internet. In a data center, these 
conditions do not apply. End-hosts are generally considered reliable and 
restricted in behaviour, which allows for great simplification of enforcement 
and prioritization policies. Admission control systems can give administrators 
the benefit of tight control over the data center, mitigating the effects of 
queue-buildup and bursts.

Although it is unclear how predictable data center traffic truly is, various 
works have shown that repeated patterns may 
exist.\cite{msr_dc,fb_dc,traffic,microte}
If data center traffic exhibits predictable patterns, it may be 
possible to devise a proactive mechanism which predicts the traffic matrix of 
next iterations, and adjusts traffic accordingly.

An ideal culmination of these insights is a global, 
centralized arbiter, able to predict and fairly distribute flows in the 
network. By only allowing a safe sending rate for each participant it minimizes 
the amount of bursts and loss possible.
Treating the network's compute and forwarding power as a single finite 
resource, the controller acts akin to a OS scheduler distributing CPU time 
slices to processes.

This project report details our exploration of such a centralized, 
proactive flow scheduler. We asked ourselves the following research questions:
\begin{enumerate}
    \item What are the requirements for such a centralized, predictive 
    scheduler to succeed?
    \item Is it possible to preemptively regulate a network by analyzing global 
    traffic patterns?
    \item What latency, packet loss and utilization are we able to achieve?
\end{enumerate}
In the scope of this report, we outline the design and implementation of our 
flow scheduler and learning approach. We perform an analysis of its 
effectiveness compared to DCTCP and Hedera and conclude with a discussion of 
the benefits and drawbacks of centralized learning and admission control in the 
data center context.